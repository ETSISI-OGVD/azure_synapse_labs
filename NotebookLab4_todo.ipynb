{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Explore and analyze data in Synapse with a serverless Apache Spark pool \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/quickstart-apache-spark-notebook#create-a-notebook \n",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Explore and analyze data in Synapse with Spark SQL - Simple data analysis by using simple data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Understanding serverless Apache Spark pools\r\n",
        "\r\n",
        "A serverless Spark pool lets you use Apache Spark without managing clusters. When you start using a pool, a Spark session is created if needed. The pool controls how many Spark resources will be used by that session and how long the session will last before it automatically pauses. You pay for spark resources used during that session and not for the pool itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create a dataframe\r\n",
        "\r\n",
        "Create a simple Spark DataFrame (3 rows x 3 colums) object to manipulate and run the cell (check whether the Spark pool you created is selected).\r\n",
        "> **Note:** If the Apache Spark pool instance isn't already running, it is automatically started. You can see the Apache Spark pool instance status below the cell you are running and also on the status panel at the bottom of the notebook. Depending on the size of pool, starting should take 2-5 minutes. Once the code has finished running, information below the cell displays showing how long it took to run and its execution. In the output cell, you see the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Next, this code creates a Spark table, a CSV, and a Parquet file all with copies of the dataframe. \r\n",
        "\r\n",
        "> [**Lectura recomendada**](https://datos.gob.es/es/blog/por-que-deberias-de-usar-ficheros-parquet-si-procesas-muchos-datos) \"Apache Parquet o simplemente Parquet es un tipo de fichero que contiene datos (de tipo tabla) en su interior, de forma similar a a un fichero tipo CSV pero no es un fichero en texto plano (se representa de forma binaria), lo que significa que no lo podemos abrir y examinar con un simple editor de texto. El formato parquet es un tipo de formato orientado-a-columnas (column-oriented file format), a diferencia de formatos orientados-a-filas (row-oriented) como CSV, TSV o AVRO.  \r\n",
        "\r\n",
        "> En un fichero CSV (recordamos, orientado a filas) cada registro es una fila. En Parquet, sin embargo, es cada columna la que se almacena de forma independiente. La diferencia más extrema la notamos cuándo, en un fichero de tipo CSV, queremos leer solamente una columna. A pesar de que solo queremos acceder a la información de una columna, por el tipo de formato, tenemos irremediablemente que leer todas las filas de la tabla. Cuando usamos formato Parquet, cada columna es accesible de forma independiente al resto. Como los datos en cada columna se espera que sean homogéneos (del mismo tipo), el formato parquet abre un sin fin de posibilidades a la hora de codificar, comprimir y optimizar el almacenamiento de los datos. De lo contrario, si lo que queremos es almacenar datos con el objetivo de leer muchas filas completas muy a menudo, el formato parquet nos penalizará en esas lecturas y no seremos eficientes ya que estamos utilizando orientación a columnas para leer filas.\r\n",
        "\r\n",
        "> Otra característica de Parquet es que es un formato de datos autodescriptivo que integra el esquema o la estructura dentro de los datos en sí. Es decir, propiedades (o metadatos) de los datos como el tipo (si es un número entero, un real o una cadena de texto), el número de valores, el tipo de compresión (los datos se pueden comprimir para ahorrar espacio), etc. están incluidas en el propio fichero junto con los datos como tal. \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "If you use the storage explorer, it's possible to see the impact of the two different ways of writing a file used above. When no file system is specified then the default is used, in this case default>user>trusted-service-user>lab4data. The data is saved to the location of the specified file system.\r\n",
        "\r\n",
        "Notice in both the \"csv\" and \"parquet\" formats, write operations creates a directory with many partitioned files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Run Spark SQL statements\r\n",
        "Next, the code lists the tables on the pool. The query retrieves the top 10 rows from a system table that comes with all Azure Synapse Apache Spark pools by default.\r\n",
        "\r\n",
        "> **Note:** When you use a Notebook with your Azure Synapse Apache Spark pool, you get a preset **sqlContext** that you can use to run queries using Spark SQL. **%%sql** tells the notebook to use the preset sqlContext to run the query. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run another query to see the data in demo_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "It is possible to get the same experience of running SQL but without having to switch languages. You can do this by replacing the SQL cell above with this PySpark cell, the output experience is the same because the display command is used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Save the notebook and end the Spark session\r\n",
        "\r\n",
        "Now that you’ve finished working with the data, you can publish the notebook with a meaningful name and end the Spark session.\r\n",
        "\r\n",
        "- In the notebook menu bar, use the Properites icon to view the notebook settings.\r\n",
        "- Set the Name of the notebook and then close the settings pane.\r\n",
        "- Below the notebook menu (right corner), select Stop session icon to end the Spark session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Clean up resources\r\n",
        "In this exercise, you’ve learned how to use Spark to work with data in Azure Synapse.\r\n",
        "\r\n",
        "If you’ve finished exploring your lakehouse, you can delete the workspace you created for this exercise."
      ]
    }
  ]
}